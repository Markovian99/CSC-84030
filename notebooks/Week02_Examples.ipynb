{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b7b635",
   "metadata": {},
   "source": [
    "## Latency vs Bandwidth: Sequential vs Random Disk Access\n",
    "\n",
    "This experiment shows how access patterns — not data size — dominate performance at scale by trading bandwidth for latency.\n",
    "\n",
    "**Note:** Random reads are aligned to fixed storage blocks (`ALIGN`)  to prevent the OS and storage controller from collapsing them into an accidental sequential stream.\n",
    "\n",
    "At scale, systems are limited by either bandwidth (sequential access) or latency (random access) — rarely both at once.\n",
    "\n",
    "Sequential access pays latency once and then streams at full bandwidth.\n",
    "Random access pays latency repeatedly and never reaches peak bandwidth.\n",
    "\n",
    "The “cost” of sequential access is not speed, but selectivity: you must scan more data to find what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1483ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random sample: 4096 MB = 1,048,576 ops × 4096 bytes\n",
      "\n",
      "Random 4KB (nocache if supported): 108.16s, avg 103.1 µs/op, effective 37.87 MB/s\n",
      "Sequential (nocache if supported): 1.07s, 3842.3 MB/s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# Parameters you can tweak\n",
    "# ----------------------------\n",
    "FILE = Path(\"bigfile.bin\")\n",
    "SIZE_GB = 4                       # size of the file to create\n",
    "SEQ_CHUNK = 8 * 1024 * 1024       # 8 MB sequential chunk\n",
    "RAND_READ_SIZE = 4096             # 4 KB random read size (classic \"small random I/O\")\n",
    "\n",
    "TOTAL_RANDOM_BYTES = SIZE_GB * 1024**3  # if too slow, reduce this\n",
    "\n",
    "ALIGN = 4096  # align random offsets to 4KB boundaries\n",
    "\n",
    "\n",
    "def make_file():\n",
    "    target = SIZE_GB * 1024**3\n",
    "    if FILE.exists() and FILE.stat().st_size >= target:\n",
    "        return\n",
    "    print(f\"Creating {SIZE_GB} GB file at {FILE} ...\")\n",
    "    with open(FILE, \"wb\") as f:\n",
    "        remaining = target\n",
    "        block = os.urandom(SEQ_CHUNK)\n",
    "        while remaining > 0:\n",
    "            n = min(SEQ_CHUNK, remaining)\n",
    "            f.write(block[:n])\n",
    "            remaining -= n\n",
    "    print(\"Done.\")\n",
    "\n",
    "def open_readonly_nocache(path: Path) -> int:\n",
    "    \"\"\"\n",
    "    Tries to open a file descriptor with OS cache disabled (macOS).\n",
    "    Falls back to normal reads on other platforms (still works, but may be cache-influenced).\n",
    "    \"\"\"\n",
    "    fd = os.open(str(path), os.O_RDONLY)\n",
    "\n",
    "    # macOS: F_NOCACHE disables caching for this fd\n",
    "    try:\n",
    "        import fcntl\n",
    "        if hasattr(fcntl, \"F_NOCACHE\"):\n",
    "            fcntl.fcntl(fd, fcntl.F_NOCACHE, 1)\n",
    "            return fd\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # If we get here, we couldn't disable cache\n",
    "    print(\"NOTE: Could not disable OS page cache on this platform.\")\n",
    "    print(\"      Results may look 'too fast' due to caching.\")\n",
    "    return fd\n",
    "\n",
    "def sequential_read_nocache() -> tuple[float, float]:\n",
    "    fd = open_readonly_nocache(FILE)\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        total = 0\n",
    "        while True:\n",
    "            b = os.read(fd, SEQ_CHUNK)\n",
    "            if not b:\n",
    "                break\n",
    "            total += len(b)\n",
    "        dt = time.perf_counter() - t0\n",
    "        mbps = total / (1024**2) / dt\n",
    "        return dt, mbps\n",
    "    finally:\n",
    "        os.close(fd)\n",
    "\n",
    "def random_reads_nocache(total_random_bytes: int) -> tuple[float, float, float]:\n",
    "    size = FILE.stat().st_size\n",
    "    n_random = total_random_bytes // RAND_READ_SIZE\n",
    "\n",
    "    fd = open_readonly_nocache(FILE)\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        total = 0\n",
    "        for _ in range(n_random):            \n",
    "            off = random.randrange(0, size - RAND_READ_SIZE, ALIGN) # choose a random aligned offset\n",
    "            b = os.pread(fd, RAND_READ_SIZE, off)\n",
    "            total += len(b)\n",
    "        dt = time.perf_counter() - t0\n",
    "\n",
    "        avg_us = (dt / n_random) * 1e6\n",
    "        eff_mbps = total / (1024**2) / dt\n",
    "        return dt, avg_us, eff_mbps\n",
    "    finally:\n",
    "        os.close(fd)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    make_file()\n",
    "\n",
    "    n_random = TOTAL_RANDOM_BYTES // RAND_READ_SIZE\n",
    "    print(f\"\\nRandom sample: {TOTAL_RANDOM_BYTES/1024**2:.0f} MB \"\n",
    "          f\"= {n_random:,} ops × {RAND_READ_SIZE} bytes\\n\")\n",
    "    \n",
    "    dt, avg_us, eff_mbps = random_reads_nocache(TOTAL_RANDOM_BYTES)\n",
    "    print(f\"Random 4KB (nocache if supported): {dt:.2f}s, avg {avg_us:.1f} µs/op, \"\n",
    "          f\"effective {eff_mbps:.2f} MB/s\")\n",
    "\n",
    "    dt, mbps = sequential_read_nocache()\n",
    "    print(f\"Sequential (nocache if supported): {dt:.2f}s, {mbps:.1f} MB/s\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a6f07",
   "metadata": {},
   "source": [
    "## From Full Scans to Partition Pruning: Why HDFS & Data Lakes Chunk Data\n",
    "\n",
    "In distributed systems like **HDFS** (Hadoop Distributed File System), large datasets are split into **chunks** (typically 128MB blocks) and spread across nodes. This design enables parallel processing, but also introduces a key optimization opportunity: **reading only the chunks you need**.\n",
    "\n",
    "### The Problem with Flat Files\n",
    "\n",
    "When you query a plain CSV, the engine must scan the **entire file** to find matching rows—even if you only want one day's worth of data from a multi-year dataset.\n",
    "\n",
    "### The Solution: Partitioning + Columnar Storage\n",
    "\n",
    "Modern data lakes solve this with two techniques:\n",
    "\n",
    "1. **Partitioning** — Physically organizing data into folders by a key column (e.g., `end_ym=2026-01/`). Queries that filter on this column can skip irrelevant partitions entirely (\"partition pruning\").\n",
    "\n",
    "2. **Columnar formats like Parquet** — Store data by column rather than row, enabling engines to read only the columns referenced in your query. Parquet also embeds min/max statistics per chunk, allowing further \"row group pruning.\"\n",
    "\n",
    "### What This Demo Shows\n",
    "\n",
    "| Step | What Happens | I/O Cost |\n",
    "|------|--------------|----------|\n",
    "| CSV scan | Read entire file, filter in memory | **High** |\n",
    "| Build partitioned Parquet | One-time ETL to reorganize data | One-time |\n",
    "| Query partitioned Parquet | Read only `end_ym==2026-01` folders | **Low** |\n",
    "\n",
    "> **Key takeaway:** The same logical query can have vastly different physical costs depending on how data is laid out on disk. This is why chunking strategies matter at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0daa76c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV scan: 1.7385910829761997 [('market_capitalization', 703)]\n",
      "Build partitioned parquet: 31.15480912500061\n",
      "Partitioned parquet: 0.447352499992121 [('market_capitalization', 703)]\n"
     ]
    }
   ],
   "source": [
    "import duckdb, time\n",
    "from pathlib import Path\n",
    "\n",
    "# Assume you have a CSV with columns: date, tickers, item, value\n",
    "CSV = \"combined_quarterly_financials.csv\"\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# 1) Query unchunked CSV (scan everything)\n",
    "t0 = time.perf_counter()\n",
    "res1 = con.execute(\"\"\"\n",
    "  SELECT item, COUNT(*)\n",
    "  FROM read_csv_auto($1)\n",
    "  WHERE end_ym == '2026-01'\n",
    "  GROUP BY item\n",
    "\"\"\", [CSV]).fetchall()\n",
    "dt1 = time.perf_counter() - t0\n",
    "print(\"CSV scan:\", dt1, res1[:5])\n",
    "\n",
    "# 2) Convert to partitioned Parquet by date (one-time cost)\n",
    "out_dir = Path(\"events_parquet\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "con.execute(f\"\"\"\n",
    "  COPY (\n",
    "    SELECT * FROM read_csv_auto('{CSV}')\n",
    "  ) TO '{out_dir.as_posix()}'\n",
    "  (FORMAT PARQUET, PARTITION_BY (end_ym));\n",
    "\"\"\")\n",
    "dt_build = time.perf_counter() - t0\n",
    "print(\"Build partitioned parquet:\", dt_build)\n",
    "\n",
    "# 3) Query partitioned Parquet (reads fewer files)\n",
    "t0 = time.perf_counter()\n",
    "res2 = con.execute(f\"\"\"\n",
    "  SELECT item, COUNT(*)\n",
    "  FROM read_parquet('{out_dir.as_posix()}/**/*.parquet')\n",
    "  WHERE end_ym == '2026-01'\n",
    "  GROUP BY item\n",
    "\"\"\").fetchall()\n",
    "dt2 = time.perf_counter() - t0\n",
    "print(\"Partitioned parquet:\", dt2, res2[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbf868",
   "metadata": {},
   "source": [
    "## Object Storage Basics: The S3 API\n",
    "\n",
    "While HDFS uses a traditional filesystem model with directories and blocks, modern data lakes increasingly rely on **object storage** systems like Amazon S3, Google Cloud Storage, or open-source alternatives like **MinIO**.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Buckets** — Top-level containers (similar to root folders)\n",
    "- **Objects** — Files stored with a unique key (path-like string) and metadata\n",
    "- **Flat namespace** — No true directories; `folder/file.txt` is just a key containing a `/`\n",
    "\n",
    "### Core S3 Operations\n",
    "\n",
    "| Operation | Description |\n",
    "|-----------|-------------|\n",
    "| `PUT` | Upload an object to a bucket |\n",
    "| `HEAD` | Retrieve metadata (size, content-type, etc.) without downloading the object |\n",
    "| `GET` | Download the full object |\n",
    "| `Range GET` | Download only a **byte range** — critical for reading chunks of large files |\n",
    "\n",
    "### Why Range GETs Matter for Big Data\n",
    "\n",
    "Columnar formats like Parquet store metadata footers at the end of files. A query engine can:\n",
    "\n",
    "1. Issue a small Range GET to read the footer\n",
    "2. Determine which row groups contain relevant data\n",
    "3. Issue targeted Range GETs for only those chunks\n",
    "\n",
    "This means you can query a 10GB Parquet file but only transfer a few MB over the network—the same \"read only what you need\" principle from HDFS, but over HTTP.\n",
    "\n",
    "### This Demo\n",
    "\n",
    "Below we use **MinIO** (an S3-compatible server running locally) to demonstrate the basic operations. The same code works against real S3 by changing the `endpoint_url` and credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3b1f323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 4294967296\n",
      "Full GET time: 18.252469667000696\n",
      "Range bytes: 1048576\n",
      "Range GET time: 17.17798308300553\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://localhost:9000\",\n",
    "    aws_access_key_id=\"minioadmin\",\n",
    "    aws_secret_access_key=\"minioadmin\",\n",
    "    config=Config(signature_version=\"s3v4\"),\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "\n",
    "bucket = \"demo\"\n",
    "try:\n",
    "    s3.create_bucket(Bucket=bucket)\n",
    "except s3.exceptions.BucketAlreadyOwnedByYou:\n",
    "    pass\n",
    "\n",
    "# PUT\n",
    "s3.upload_file(\"bigfile.bin\", bucket, \"bigfile.bin\")\n",
    "\n",
    "# HEAD (metadata)\n",
    "head = s3.head_object(Bucket=bucket, Key=\"bigfile.bin\")\n",
    "print(\"Size:\", head[\"ContentLength\"])\n",
    "\n",
    "# GET (full)\n",
    "start = time.perf_counter()\n",
    "s3.download_file(bucket, \"bigfile.bin\", \"bigfile_downloaded.bin\")\n",
    "print(\"Full GET time:\", time.perf_counter() - start)\n",
    "\n",
    "# Range GET (partial read)\n",
    "resp = s3.get_object(Bucket=bucket, Key=\"bigfile.bin\", Range=\"bytes=0-1048575\")\n",
    "data = resp[\"Body\"].read()\n",
    "print(\"Range bytes:\", len(data))\n",
    "\n",
    "# do many get ranges to get the full object\n",
    "start_time = time.perf_counter()\n",
    "size = head[\"ContentLength\"]\n",
    "chunk_size = 4*1024*1024  # 4 MB\n",
    "n_chunks = size // chunk_size\n",
    "for i in range(n_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = start + chunk_size - 1\n",
    "    resp = s3.get_object(Bucket=bucket, Key=\"bigfile.bin\", Range=f\"bytes={start}-{end}\")\n",
    "    data = resp[\"Body\"].read()\n",
    "print(\"Range GET time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc24a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuny312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
